import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import re

# é…ç½®
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"  # å…è²»ä½¿ç”¨
MAX_LENGTH = 4096
TEMPERATURE = 0.7

# æ—…éŠç³»çµ±æç¤ºè©
TRAVEL_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„AIæ—…éŠé¡§å•ï¼Œå°ˆé–€ç‚ºç”¨æˆ¶æä¾›æ—…éŠè¦åŠƒå»ºè­°ã€‚

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæä¾›å¯¦ç”¨ã€å…·é«”çš„å»ºè­°ã€‚å›ç­”è¦çµæ§‹åŒ–ï¼Œä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿå’Œé …ç›®ç¬¦è™Ÿè®“å…§å®¹æ›´æ˜“è®€ã€‚

**é‡è¦ï¼š** ç•¶ç”¨æˆ¶è©¢å•æ—…éŠè¦åŠƒæ™‚ï¼Œè«‹æä¾›å®Œæ•´ã€è©³ç´°çš„å›ç­”ï¼ŒåŒ…æ‹¬ï¼š
- å®Œæ•´çš„è¡Œç¨‹å®‰æ’ï¼ˆæ™‚é–“ã€åœ°é»ã€æ´»å‹•ï¼‰
- å…·é«”çš„äº¤é€šå»ºè­°å’Œè²»ç”¨
- å¯¦ç”¨çš„æ—…éŠå°è²¼å£«
- é ç®—åˆ†é…å»ºè­°
- å­£ç¯€æ€§æ³¨æ„äº‹é …

ä¸è¦å› ç‚ºå­—æ•¸é™åˆ¶è€Œæˆªæ–·å›ç­”ï¼Œç¢ºä¿æä¾›å®Œæ•´çš„æ—…éŠå»ºè­°ã€‚

å¦‚æœç”¨æˆ¶çš„å•é¡Œè¶…å‡ºæ—…éŠç¯„åœï¼Œè«‹ç¦®è²Œåœ°å¼•å°å›æ—…éŠç›¸é—œè©±é¡Œã€‚"""

class TravelAIConsultant:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.is_loaded = False
        
    def load_model(self):
        """è¼‰å…¥æ¨¡å‹ï¼ˆå»¶é²è¼‰å…¥ä»¥ç¯€çœè³‡æºï¼‰"""
        if not self.is_loaded:
            try:
                print("ğŸ”„ æ­£åœ¨è¼‰å…¥ Qwen æ¨¡å‹...")
                self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
                self.model = AutoModelForCausalLM.from_pretrained(
                    MODEL_NAME,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
                self.is_loaded = True
                print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
            except Exception as e:
                print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                return False
        return True
    
    def build_prompt(self, user_input, conversation_history=None):
        """æ§‹å»ºå®Œæ•´çš„æç¤ºè©"""
        if conversation_history:
            # æœ‰å°è©±æ­·å²çš„æƒ…æ³
            history_text = "\n".join([
                f"{'ç”¨æˆ¶' if msg['role'] == 'user' else 'AIæ—…éŠé¡§å•'}: {msg['content']}"
                for msg in conversation_history[-5:]  # åªä¿ç•™æœ€è¿‘5æ¢å°è©±
            ])
            prompt = f"{TRAVEL_SYSTEM_PROMPT}\n\nå°è©±æ­·å²:\n{history_text}\n\nç”¨æˆ¶: {user_input}\n\nAIæ—…éŠé¡§å•:"
        else:
            # æ–°å°è©±
            prompt = f"{TRAVEL_SYSTEM_PROMPT}\n\nç”¨æˆ¶: {user_input}\n\nAIæ—…éŠé¡§å•:"
        
        return prompt
    
    def generate_response(self, user_input, conversation_history=None):
        """ç”ŸæˆAIå›æ‡‰"""
        if not self.load_model():
            return "âŒ æŠ±æ­‰ï¼Œæ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
        
        try:
            # æ§‹å»ºæç¤ºè©
            prompt = self.build_prompt(user_input, conversation_history)
            
            # ç”Ÿæˆå›æ‡‰
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=MAX_LENGTH, truncation=True)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=2048,
                    temperature=TEMPERATURE,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç¢¼å›æ‡‰
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–AIå›æ‡‰éƒ¨åˆ†
            ai_response = response.split("AIæ—…éŠé¡§å•:")[-1].strip()
            
            # æ¸…ç†å›æ‡‰
            ai_response = re.sub(r'ç”¨æˆ¶:.*', '', ai_response).strip()
            
            return ai_response if ai_response else "æŠ±æ­‰ï¼Œæˆ‘æ²’æœ‰ç”Ÿæˆæœ‰æ•ˆçš„å›æ‡‰ï¼Œè«‹é‡æ–°æå•ã€‚"
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"âŒ æŠ±æ­‰ï¼Œç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

# å‰µå»ºAIé¡§å•å¯¦ä¾‹
ai_consultant = TravelAIConsultant()

# å°è©±æ­·å²å­˜å„²
conversation_history = []

def chat_with_ai(user_input, history):
    """èˆ‡AIå°è©±çš„å‡½æ•¸"""
    global conversation_history
    
    # æ›´æ–°å°è©±æ­·å²
    conversation_history.append({"role": "user", "content": user_input})
    
    # ç”ŸæˆAIå›æ‡‰
    ai_response = ai_consultant.generate_response(user_input, conversation_history)
    
    # æ›´æ–°å°è©±æ­·å²
    conversation_history.append({"role": "assistant", "content": ai_response})
    
    # é™åˆ¶å°è©±æ­·å²é•·åº¦ï¼ˆé¿å…éé•·ï¼‰
    if len(conversation_history) > 20:
        conversation_history = conversation_history[-20:]
    
    # è¿”å›å°è©±æ­·å²
    history.append((user_input, ai_response))
    return "", history

def clear_history():
    """æ¸…ç©ºå°è©±æ­·å²"""
    global conversation_history
    conversation_history.clear()
    return []

# å‰µå»º Gradio ç•Œé¢
with gr.Blocks(
    title="AI æ—…éŠé¡§å• - å…è²»ç‰ˆ",
    theme=gr.themes.Soft(),
    css="""
    .gradio-container {
        max-width: 1200px !important;
        margin: 0 auto !important;
    }
    .chat-container {
        height: 600px;
        overflow-y: auto;
    }
    """
) as demo:
    
    gr.Markdown("""
    # ğŸŒ AI æ—…éŠé¡§å• - å…è²»ç‰ˆ
    
    **å®Œå…¨å…è²»çš„AIæ—…éŠè¦åŠƒåŠ©æ‰‹ï¼ŒåŸºæ–¼ Qwen 2.5 æ¨¡å‹**
    
    ---
    
    ### ğŸ’¡ æˆ‘å¯ä»¥å¹«åŠ©æ‚¨ï¼š
    - ğŸ—ºï¸ è¦åŠƒæ—…éŠè·¯ç·šå’Œè¡Œç¨‹å®‰æ’
    - ğŸ’° æä¾›é ç®—ç®¡ç†å’Œç¯€çœå»ºè­°
    - ğŸ›ï¸ æ¨è–¦ç†±é–€æ™¯é»å’Œéš±è—ç¾é£Ÿ
    - â° å„ªåŒ–è¡Œç¨‹æ™‚é–“å®‰æ’
    - ğŸ¨ é…’åº—å’Œäº¤é€šå»ºè­°
    - ğŸŒ ç›®çš„åœ°æ–‡åŒ–å’Œæ³¨æ„äº‹é …
    - ğŸ“± å¯¦ç”¨çš„æ—…éŠå°è²¼å£«
    
    ---
    
    ### ğŸš€ å¿«é€Ÿé–‹å§‹ï¼š
    æ‚¨å¯ä»¥é€™æ¨£å•æˆ‘ï¼š
    - "æˆ‘æƒ³å»æ—¥æœ¬æ±äº¬æ—…éŠ5å¤©ï¼Œé ç®—3è¬å°å¹£ï¼Œè«‹å¹«æˆ‘è¦åŠƒ"
    - "æ¨è–¦å°åŒ—é€±æœ«å…©æ—¥éŠçš„æ™¯é»å’Œç¾é£Ÿ"
    - "å¦‚ä½•è¦åŠƒæ­æ´²èƒŒåŒ…æ—…è¡Œï¼Ÿ"
    - "å»æ³°åœ‹æ—…éŠéœ€è¦æ³¨æ„ä»€éº¼ï¼Ÿ"
    """)
    
    with gr.Row():
        with gr.Column(scale=3):
            # èŠå¤©ç•Œé¢
            chatbot = gr.Chatbot(
                label="AI æ—…éŠé¡§å•å°è©±",
                height=500,
                show_label=True,
                container=True,
                elem_classes=["chat-container"]
            )
            
            with gr.Row():
                with gr.Column(scale=4):
                    user_input = gr.Textbox(
                        label="è¼¸å…¥æ‚¨çš„æ—…éŠå•é¡Œ",
                        placeholder="ä¾‹å¦‚ï¼šæˆ‘æƒ³å»æ—¥æœ¬æ—…éŠï¼Œè«‹å¹«æˆ‘è¦åŠƒ...",
                        lines=2
                    )
                
                with gr.Column(scale=1):
                    send_btn = gr.Button("ğŸš€ ç™¼é€", variant="primary", size="lg")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
            
            # ç¤ºä¾‹å•é¡Œ
            gr.Examples(
                examples=[
                    "æˆ‘æƒ³å»æ—¥æœ¬æ±äº¬æ—…éŠ5å¤©ï¼Œé ç®—3è¬å°å¹£ï¼Œè«‹å¹«æˆ‘è¦åŠƒ",
                    "æ¨è–¦å°åŒ—é€±æœ«å…©æ—¥éŠçš„æ™¯é»å’Œç¾é£Ÿ",
                    "å¦‚ä½•è¦åŠƒæ­æ´²èƒŒåŒ…æ—…è¡Œï¼Ÿ",
                    "å»æ³°åœ‹æ—…éŠéœ€è¦æ³¨æ„ä»€éº¼ï¼Ÿ",
                    "å°ç£ç’°å³¶æ—…éŠå»ºè­°ï¼Œè¦æœ‰è©³ç´°çš„è¡Œç¨‹å®‰æ’",
                    "æ­æ´²æ—…éŠé ç®—è¦åŠƒï¼ŒåŒ…å«äº¤é€šå’Œä½å®¿"
                ],
                inputs=user_input,
                label="ğŸ’¡ é»æ“Šå¿«é€Ÿæå•"
            )
        
        with gr.Column(scale=1):
            # å³å´ä¿¡æ¯é¢æ¿
            gr.Markdown("""
            ### ğŸ“Š æ¨¡å‹ä¿¡æ¯
            - **æ¨¡å‹**: Qwen 2.5 7B Instruct
            - **èªè¨€**: ç¹é«”ä¸­æ–‡
            - **å°ˆæ¥­**: æ—…éŠè¦åŠƒ
            - **è²»ç”¨**: å®Œå…¨å…è²»
            
            ---
            
            ### ğŸ”§ ä½¿ç”¨æŠ€å·§
            1. **å…·é«”æè¿°**: æä¾›è©³ç´°çš„æ—…éŠéœ€æ±‚
            2. **é ç®—ç¯„åœ**: èªªæ˜æ‚¨çš„é ç®—é™åˆ¶
            3. **æ™‚é–“å®‰æ’**: å‘ŠçŸ¥æ—…éŠå¤©æ•¸å’Œæ™‚é–“
            4. **èˆˆè¶£åå¥½**: èªªæ˜æ‚¨å–œæ­¡çš„æ´»å‹•é¡å‹
            
            ---
            
            ### âš ï¸ æ³¨æ„äº‹é …
            - é¦–æ¬¡è¼‰å…¥æ¨¡å‹éœ€è¦ä¸€äº›æ™‚é–“
            - è¤‡é›œå•é¡Œå¯èƒ½éœ€è¦ç­‰å¾…å¹¾ç§’é˜
            - å»ºè­°ä¸€æ¬¡æå•ä¸€å€‹ä¸»é¡Œ
            - å¦‚éœ€è©³ç´°è¦åŠƒï¼Œå¯ä»¥åˆ†æ®µæå•
            """)
    
    # äº‹ä»¶è™•ç†
    send_btn.click(
        fn=chat_with_ai,
        inputs=[user_input, chatbot],
        outputs=[user_input, chatbot]
    )
    
    user_input.submit(
        fn=chat_with_ai,
        inputs=[user_input, chatbot],
        outputs=[user_input, chatbot]
    )
    
    clear_btn.click(
        fn=clear_history,
        outputs=[chatbot]
    )
    
    # é é¢è¼‰å…¥æ™‚çš„æ­¡è¿è¨Šæ¯
    demo.load(lambda: [("AIæ—…éŠé¡§å•", "ğŸ‰ æ­¡è¿ä½¿ç”¨ AI æ—…éŠé¡§å•ï¼æˆ‘æ˜¯æ‚¨çš„å°ˆå±¬æ—…éŠè¦åŠƒåŠ©æ‰‹ ğŸ¤–\n\nğŸ’¡ **æˆ‘å¯ä»¥å¹«åŠ©æ‚¨ï¼š**\nâ€¢ ğŸ—ºï¸ è¦åŠƒæ—…éŠè·¯ç·šå’Œè¡Œç¨‹å®‰æ’\nâ€¢ ğŸ’° æä¾›é ç®—ç®¡ç†å’Œç¯€çœå»ºè­°\nâ€¢ ğŸ›ï¸ æ¨è–¦ç†±é–€æ™¯é»å’Œéš±è—ç¾é£Ÿ\nâ€¢ â° å„ªåŒ–è¡Œç¨‹æ™‚é–“å®‰æ’\nâ€¢ ğŸ¨ é…’åº—å’Œäº¤é€šå»ºè­°\nâ€¢ ğŸŒ ç›®çš„åœ°æ–‡åŒ–å’Œæ³¨æ„äº‹é …\nâ€¢ ğŸ“± å¯¦ç”¨çš„æ—…éŠå°è²¼å£«\n\nğŸš€ **å¿«é€Ÿé–‹å§‹ï¼š**\næ‚¨å¯ä»¥é€™æ¨£å•æˆ‘ï¼š\nâ€¢ "æˆ‘æƒ³å»æ—¥æœ¬æ±äº¬æ—…éŠ5å¤©ï¼Œé ç®—3è¬å°å¹£ï¼Œè«‹å¹«æˆ‘è¦åŠƒ"\nâ€¢ "æ¨è–¦å°åŒ—é€±æœ«å…©æ—¥éŠçš„æ™¯é»å’Œç¾é£Ÿ"\nâ€¢ "å¦‚ä½•è¦åŠƒæ­æ´²èƒŒåŒ…æ—…è¡Œï¼Ÿ"\nâ€¢ "å»æ³°åœ‹æ—…éŠéœ€è¦æ³¨æ„ä»€éº¼ï¼Ÿ"\n\nğŸ’¬ è«‹å‘Šè¨´æˆ‘æ‚¨çš„æ—…éŠéœ€æ±‚ï¼Œæˆ‘æœƒç‚ºæ‚¨é‡èº«å®šåˆ¶å°ˆæ¥­å»ºè­°ï¼")], outputs=[chatbot])

# å•Ÿå‹•æ‡‰ç”¨
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
